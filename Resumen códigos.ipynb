{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repaso parcial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías básicas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Justicia\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap  # Para interpretabilidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargar y procesar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazar con los datos que te proporcionen\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Dividir en características (X) y etiqueta (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# División de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Escalar si es necesario\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo base de regresión\n",
    "reg_model = RandomForestRegressor(random_state=42)\n",
    "reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = reg_model.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"R2:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# Interpretabilidad\n",
    "feature_importances = reg_model.feature_importances_\n",
    "plt.barh(X.columns, feature_importances)\n",
    "plt.title(\"Importancia de Características\")\n",
    "plt.show()\n",
    "\n",
    "# SHAP para explicabilidad\n",
    "explainer = shap.Explainer(reg_model, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelos de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo base de clasificación\n",
    "clf_model = RandomForestClassifier(random_state=42)\n",
    "clf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred = clf_model.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Interpretabilidad\n",
    "feature_importances = clf_model.feature_importances_\n",
    "plt.barh(X.columns, feature_importances)\n",
    "plt.title(\"Importancia de Características\")\n",
    "plt.show()\n",
    "\n",
    "# SHAP para explicabilidad\n",
    "explainer = shap.Explainer(clf_model, X_test)\n",
    "shap_values = explainer(X_test)\n",
    "shap.summary_plot(shap_values, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparación de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo: GridSearch para optimizar hiperparámetros\n",
    "param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Validar si mejora es significativa\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix (Mejor Modelo):\\n\", confusion_matrix(y_test, y_pred_best))\n",
    "print(\"ROC AUC (Mejor Modelo):\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Justicia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar justicia con variables sensibles\n",
    "sensitive_feature = 'gender'  # Ejemplo: columna sensible\n",
    "group_a = df[df[sensitive_feature] == 0]\n",
    "group_b = df[df[sensitive_feature] == 1]\n",
    "\n",
    "# Comparar métricas entre grupos\n",
    "for group, data in {'Group A': group_a, 'Group B': group_b}.items():\n",
    "    y_true = data['target']\n",
    "    y_pred = clf_model.predict(data.drop('target', axis=1))\n",
    "    print(f\"{group} Metrics\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Despliegue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo para despliegue\n",
    "import joblib\n",
    "joblib.dump(best_model, 'best_model.pkl')\n",
    "\n",
    "# Cargar y predecir con modelo desplegado\n",
    "model = joblib.load('best_model.pkl')\n",
    "predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pruebas A/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_absolute_error\n",
    "from scipy.stats import ttest_rel, wilcoxon\n",
    "\n",
    "# Paso 1: Entrenar dos modelos\n",
    "# Modelo A (Base)\n",
    "model_a = RandomForestClassifier(random_state=42)\n",
    "model_a.fit(X_train, y_train)\n",
    "y_pred_a = model_a.predict(X_test)\n",
    "\n",
    "# Modelo B (Mejorado)\n",
    "model_b = LogisticRegression(max_iter=1000, random_state=42)\n",
    "model_b.fit(X_train, y_train)\n",
    "y_pred_b = model_b.predict(X_test)\n",
    "\n",
    "# Paso 2: Evaluar los modelos\n",
    "# (Cambia a las métricas correspondientes si es un problema de regresión)\n",
    "accuracy_a = accuracy_score(y_test, y_pred_a)\n",
    "accuracy_b = accuracy_score(y_test, y_pred_b)\n",
    "\n",
    "print(f\"Accuracy Modelo A: {accuracy_a}\")\n",
    "print(f\"Accuracy Modelo B: {accuracy_b}\")\n",
    "\n",
    "# Paso 3: Comparar predicciones (Prueba A/B)\n",
    "# Métrica para cada ejemplo individual\n",
    "errors_a = y_test != y_pred_a\n",
    "errors_b = y_test != y_pred_b\n",
    "\n",
    "# T-Test pareado (diferencias medias)\n",
    "t_stat, p_value = ttest_rel(errors_a, errors_b)\n",
    "print(f\"T-Test: t-statistic = {t_stat}, p-value = {p_value}\")\n",
    "\n",
    "# Prueba de Wilcoxon (no paramétrica)\n",
    "wilcoxon_stat, wilcoxon_p = wilcoxon(errors_a, errors_b)\n",
    "print(f\"Wilcoxon: stat = {wilcoxon_stat}, p-value = {wilcoxon_p}\")\n",
    "\n",
    "# Interpretación de resultados\n",
    "if p_value < 0.05:\n",
    "    print(\"La diferencia entre los modelos es estadísticamente significativa (T-Test).\")\n",
    "else:\n",
    "    print(\"No hay evidencia suficiente para afirmar que los modelos son diferentes (T-Test).\")\n",
    "\n",
    "if wilcoxon_p < 0.05:\n",
    "    print(\"La diferencia entre los modelos es estadísticamente significativa (Wilcoxon).\")\n",
    "else:\n",
    "    print(\"No hay evidencia suficiente para afirmar que los modelos son diferentes (Wilcoxon).\")\n",
    "\n",
    "# Paso 4: Visualizar diferencias en desempeño\n",
    "# Porcentaje de aciertos por modelo\n",
    "errors_a_mean = np.mean(errors_a)\n",
    "errors_b_mean = np.mean(errors_b)\n",
    "\n",
    "plt.bar(['Modelo A', 'Modelo B'], [1-errors_a_mean, 1-errors_b_mean])\n",
    "plt.ylabel('Proporción de aciertos')\n",
    "plt.title('Comparación entre modelos')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular errores absolutos en regresión\n",
    "errors_a = np.abs(y_test - model_a.predict(X_test))\n",
    "errors_b = np.abs(y_test - model_b.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supuestos de negocio\n",
    "income_per_correct_prediction = 100  # Ingreso por predicción correcta\n",
    "cost_per_error = 50  # Costo por predicción incorrecta\n",
    "model_cost = 10000  # Costo de desarrollo e implementación\n",
    "\n",
    "# Ingresos y costos del Modelo A\n",
    "correct_a = sum(y_test == y_pred_a)\n",
    "incorrect_a = sum(y_test != y_pred_a)\n",
    "income_a = correct_a * income_per_correct_prediction\n",
    "cost_a = incorrect_a * cost_per_error + model_cost\n",
    "roi_a = (income_a - cost_a) / model_cost\n",
    "\n",
    "# Ingresos y costos del Modelo B\n",
    "correct_b = sum(y_test == y_pred_b)\n",
    "incorrect_b = sum(y_test != y_pred_b)\n",
    "income_b = correct_b * income_per_correct_prediction\n",
    "cost_b = incorrect_b * cost_per_error + model_cost\n",
    "roi_b = (income_b - cost_b) / model_cost\n",
    "\n",
    "# Punto de Equilibrio para el Modelo Mejorado\n",
    "break_even_correct_predictions = model_cost / income_per_correct_prediction\n",
    "\n",
    "# Resultados\n",
    "print(f\"Modelo A - Ingreso: ${income_a}, Costo: ${cost_a}, ROI: {roi_a:.2f}\")\n",
    "print(f\"Modelo B - Ingreso: ${income_b}, Costo: ${cost_b}, ROI: {roi_b:.2f}\")\n",
    "print(f\"Punto de Equilibrio (Modelo Mejorado): {break_even_correct_predictions:.0f} predicciones correctas\")\n",
    "\n",
    "# Comparación Visual\n",
    "models = ['Modelo A', 'Modelo B']\n",
    "rois = [roi_a, roi_b]\n",
    "\n",
    "plt.bar(models, rois)\n",
    "plt.ylabel('ROI')\n",
    "plt.title('Comparación de ROI entre Modelos')\n",
    "plt.axhline(0, color='red', linestyle='--', label='Punto de Equilibrio')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
